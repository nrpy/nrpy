#include "BHaH_defines.h"
#include "BHaH_function_prototypes.h"
#include "diagnostics/diagnostic_gfs.h"

/**
 * @file diagnostics.c
 * @brief Top-level driver that schedules and runs all enabled diagnostics.
 *
 * The function "diagnostics" is generated by NRPy and invoked once per timestep.
 * It checks whether the current time falls on a diagnostics output step. On output
 * steps it allocates per-grid temporary arrays (diagnostic_gfs), initializes them
 * via diagnostic_gfs_set(...), and runs the enabled diagnostics families
 * (for example diagnostics_nearest(...), diagnostics_interp(...), and
 * diagnostics_volume_integration(...)). Temporary storage is freed before
 * returning. Independently of output steps, a progress indicator is advanced
 * every call, and a trailing newline is printed when the run is about to finish.
 *
 * Scheduling rule:
 *   fabs(round(time / diagnostics_output_every) * diagnostics_output_every - time) < 0.5 * dt
 *
 * CUDA note: When compiled with CUDA, device-to-host synchronization of selected
 * buffers occurs prior to performing diagnostics that require host-side I/O, and
 * an additional griddata_device parameter is present in the signature.
 *
 * @pre
 * - commondata and griddata are non-null and initialized.
 * - commondata->NUMGRIDS >= 1, grid dimensions are valid, and backing arrays exist.
 * - Generated diagnostics interfaces and indices are consistent with the build.
 *
 * @post
 * - On output steps, all enabled diagnostics execute and may write output.
 * - On non-output steps, no diagnostic I/O occurs and the solution state is unchanged.
 * - The progress indicator advances every call; a newline is printed if time + dt > t_final.
 *
 * @param[in,out] commondata  Global simulation metadata and run-time parameters
 *                            (e.g., time, dt, diagnostics_output_every, t_final, NUMGRIDS).
 * @param[in,out] griddata_device  Device-side per-grid data used for device-to-host
 *                                 synchronization when compiled with CUDA; omitted in non-CUDA builds.
 * @param[in,out] griddata    Host-side per-grid data (parameters, fields, and workspace).
 *
 * @warning
 * - Diagnostics that encounter allocation or I/O failures may abort the program.
 * - The set of diagnostics compiled in is fixed at code generation time; manual changes
 *   must remain consistent with generated headers and prototypes.
 *
 * If a user-editable block is provided by the build, it may be used to add custom
 * diagnostics or I/O behaviors; details are intentionally omitted here.
 *
 * @return void
 */
void diagnostics(commondata_struct *restrict commondata, griddata_struct *restrict griddata) {
  const REAL currtime = commondata->time, currdt = commondata->dt, outevery = commondata->diagnostics_output_every;
  // Explanation of the if() below:
  // Step 1: round(currtime / outevery) rounds to the nearest integer multiple of currtime/outevery.
  // Step 2: Multiplying by outevery yields the exact time we should output again, t_out.
  // Step 3: If fabs(t_out - currtime) < 0.5 * currdt, then currtime is as close to t_out as possible!
  if (fabs(round(currtime / outevery) * outevery - currtime) < 0.5 * currdt) {
    // Diagnostics require additional memory, so first free all scratch storage needed for MoL timestepping.
    // FIXME: will address this later. Make code work first, then optimize later.
    // for (int grid = 0; grid < commondata->NUMGRIDS; grid++)
    //   MoL_free_intermediate_stage_gfs(&griddata[grid].gridfuncs);

    // Allocate temporary storage for diagnostic_gfs.
    REAL *diagnostic_gfs[MAXNUMGRIDS];
    for (int grid = 0; grid < commondata->NUMGRIDS; grid++) {
      SET_NXX_PLUS_2NGHOSTS_VARS(grid);
      const int Nxx_plus_2NGHOSTS_tot = Nxx_plus_2NGHOSTS0 * Nxx_plus_2NGHOSTS1 * Nxx_plus_2NGHOSTS2;
      BHAH_MALLOC(diagnostic_gfs[grid], TOTAL_NUM_DIAG_GFS * Nxx_plus_2NGHOSTS_tot * sizeof(REAL));

#ifdef __CUDACC__
      // This does not leverage async memory transfers using multiple streams at the moment
      // given the current intent is one cuda stream per grid. This could be leveraged
      // in the future by increasing NUM_STREAMS such that a diagnostic stream is included per grid
      const params_struct *restrict params = &griddata_device[grid].params;
      size_t streamid = params->grid_idx % NUM_STREAMS;
      cpyHosttoDevice_params__constant(&griddata_device[grid].params, streamid);
      // Copy solution to host
      for (int gf = 0; gf < NUM_EVOL_GFS; gf++)
        cpyDevicetoHost__gf(commondata, params, griddata[grid].gridfuncs.y_n_gfs, griddata_device[grid].gridfuncs.y_n_gfs, gf, gf, streamid);
      // Sync data before attempting to write to file
      cudaStreamSynchronize(streams[streamid]);
#endif // __CUDACC__
    } // END LOOP over grids

    // Set diagnostics_gfs -- see nrpy/infrastructures/BHaH/[project]/diagnostics/ for definition.
    diagnostic_gfs_set(commondata, griddata, diagnostic_gfs);

    // Nearest-point diagnostics, at center, along y,z axes (1D) and xy and yz planes (2D).
    diagnostics_nearest(commondata, griddata, (const double **)diagnostic_gfs);

    // Volume-integration diagnostics.
    diagnostics_volume_integration(commondata, griddata, (const double **)diagnostic_gfs);

    // Free temporary storage allocated to diagnostic_gfs.
    for (int grid = 0; grid < commondata->NUMGRIDS; grid++)
      free(diagnostic_gfs[grid]);

    // Re-allocate all scratch storage needed for resumption of MoL timestepping.
    // FIXME: will address this later. Make code work first, then optimize later.
    // for (int grid = 0; grid < commondata->NUMGRIDS; grid++)
    //   MoL_malloc_intermediate_stage_gfs(commondata, &griddata[grid].params, &griddata[grid].gridfuncs);
  } // END if output step

  progress_indicator(commondata, griddata);
  if (commondata->time + commondata->dt > commondata->t_final)
    printf("\n");
} // END FUNCTION diagnostics
